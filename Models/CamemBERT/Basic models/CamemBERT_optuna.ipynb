{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install transformers torch datasets optuna scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from transformers import Trainer, TrainingArguments, CamembertTokenizer, CamembertForSequenceClassification\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorWithPadding, TrainerCallback, TrainerState, TrainerControl\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import pipeline\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments, CamembertTokenizer, CamembertForSequenceClassification\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train data\n",
    "train = pd.read_csv('training_data.csv')\n",
    "train.set_index('id', inplace=True)\n",
    "display(train.head())\n",
    "\n",
    "#Import the unlabel data\n",
    "unlabel = pd.read_csv('unlabelled_test_data.csv')\n",
    "unlabel.set_index('id', inplace=True)\n",
    "display(unlabel.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Labedncoder\n",
    "diffuculty_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels\n",
    "train['difficulty'] = diffuculty_encoder.fit_transform(train['difficulty'])\n",
    "\n",
    "# Print the classes and their corresponding encoded values\n",
    "for index, label in enumerate(diffuculty_encoder.classes_):\n",
    "    print(f'{label}: {index}')\n",
    "\n",
    "\n",
    "#Display train\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CamemBERT tokenizer and model\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    result = tokenizer(examples['sentence'], padding=\"max_length\", truncation=True)\n",
    "    # Ensure labels are included if they exist in the examples\n",
    "    if 'difficulty' in examples:\n",
    "        result['labels'] = examples['difficulty']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and validation sets\n",
    "train_df, val_df = train_test_split(train, test_size=0.1)\n",
    "\n",
    "display(train_df.head(1))\n",
    "display(val_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas dataframe to Hugging Face dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Tokenize the data\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization with optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# Define the RDStopping callback\n",
    "class RDStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, threshold=0.01, patience=3):\n",
    "        self.threshold = threshold\n",
    "        self.patience = patience\n",
    "        self.best_loss = None\n",
    "        self.consecutive_below_threshold = 0\n",
    "\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        # Get the current validation loss\n",
    "        logs = kwargs.get(\"metrics\", {})\n",
    "        val_loss = logs.get(\"eval_loss\", None)\n",
    "\n",
    "        if val_loss is not None:\n",
    "            if self.best_loss is None:\n",
    "                self.best_loss = val_loss\n",
    "\n",
    "            # Calculate relative divergence\n",
    "            relative_divergence = abs((self.best_loss - val_loss) / self.best_loss)\n",
    "\n",
    "            if relative_divergence < self.threshold:\n",
    "                self.consecutive_below_threshold += 1\n",
    "                print(f\"Relative divergence below threshold: {relative_divergence:.4f} for {self.consecutive_below_threshold} consecutive evaluations.\")\n",
    "            else:\n",
    "                self.consecutive_below_threshold = 0\n",
    "\n",
    "            # Update the best loss\n",
    "            if val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "\n",
    "            # Stop training if the threshold is not met for `patience` evaluations\n",
    "            if self.consecutive_below_threshold >= self.patience:\n",
    "                control.should_training_stop = True\n",
    "                print(f\"Stopping training as the relative divergence has been below the threshold for {self.patience} consecutive evaluations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DataCollatorWithPadding to pad the sequences in a batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# Define RDStopping callback\n",
    "rd_stopping_callback = [RDStoppingCallback(threshold=0.01, patience=3)]\n",
    "\n",
    "best_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-4)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32, 64])\n",
    "    num_train_epochs = trial.suggest_int('num_train_epochs', 3, 10)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 0.1)\n",
    "    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2, 4])\n",
    "    warmup_steps = trial.suggest_categorical('warmup_steps', [0, 100, 200, 500])\n",
    "    lr_scheduler_type = trial.suggest_categorical('lr_scheduler_type', ['linear', 'cosine', 'cosine_with_restarts'])\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        weight_decay=weight_decay,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        warmup_steps=warmup_steps,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        save_steps=500,\n",
    "        save_total_limit=3,\n",
    "        seed=42,\n",
    "        fp16=use_gpu,\n",
    "        no_cuda=not use_gpu\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks= rd_stopping_callback\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    eval_loss = eval_results['eval_loss']\n",
    "\n",
    "    # Save model and hyperparameters if it is one of the top 5\n",
    "    if len(best_models) < 5:\n",
    "        best_models.append((eval_loss, trial.params, f'./results/{trial.number}'))\n",
    "        best_models.sort(key=lambda x: x[0])\n",
    "    elif eval_loss < best_models[-1][0]:\n",
    "        # Remove the worst model\n",
    "        worst_model_path = best_models.pop()[2]\n",
    "        if os.path.exists(worst_model_path):\n",
    "            os.system(f'rm -r {worst_model_path}')\n",
    "        best_models.append((eval_loss, trial.params, f'./results/{trial.number}'))\n",
    "        best_models.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Return the evaluation loss for optimization\n",
    "    return eval_results['eval_loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a study and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)  # Adjust the number of trials as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best Hyperparameters: {study.best_params}\")\n",
    "# Print the best hyperparameters and corresponding loss\n",
    "for i, (loss, params, model_path) in enumerate(best_models):\n",
    "    print(f\"Model {i+1} - Loss: {loss}, Hyperparameters: {params}, Model Path: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the top 5 models and their tokenizers\n",
    "model_paths = [\n",
    "    './saved_models/CamemBERT_V1',\n",
    "    './saved_models/CamemBERT_V2',\n",
    "    './saved_models/CamemBERT_V3',\n",
    "    './saved_models/CamemBERT_V4',\n",
    "    './saved_models/CamemBERT_V5'\n",
    "]\n",
    "\n",
    "# Define the label names in the order of their corresponding indices (0 to 5)\n",
    "label_names = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
    "\n",
    "display(unlabel.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the difficulty of a single sentence using a specific model and tokenizer\n",
    "def predict_difficulty(nlp, sentence):\n",
    "    results = nlp(sentence)\n",
    "    predictions = [{label_names[i]: score for i, score in enumerate(result)} for result in results]\n",
    "    best_prediction = max(predictions[0], key=lambda key: predictions[0][key]['score'])\n",
    "    return best_prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the models and tokenizers\n",
    "for model_path in model_paths:\n",
    "    model_name = model_path.split('/')[-1]  # Extract model name from path\n",
    "    model = CamembertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = CamembertTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Create a pipeline for text classification\n",
    "    nlp = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "    \n",
    "    # Apply the prediction function to each sentence in the 'sentence' column of unlabel DataFrame\n",
    "    unlabel['difficulty'] = unlabel['sentence'].apply(lambda sentence: predict_difficulty(nlp, sentence))\n",
    "    \n",
    "    # Export the predictions to a CSV file\n",
    "    output_file = f'/mnt/data/{model_name}.csv'\n",
    "    unlabel.drop('sentence', axis=1, inplace=True)  # Remove the column 'sentence' for the output\n",
    "    unlabel.to_csv(output_file, index=False)\n",
    "\n",
    "    # Reload the original unlabelled data for the next iteration\n",
    "    unlabel = pd.read_csv('/mnt/data/unlabelled_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
