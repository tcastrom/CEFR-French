{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Les coûts kilométriques réels peuvent diverger...</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le bleu, c'est ma couleur préférée mais je n'a...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Le test de niveau en français est sur le site ...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Est-ce que ton mari est aussi de Boston?</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dans les écoles de commerce, dans les couloirs...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence difficulty\n",
       "id                                                              \n",
       "0   Les coûts kilométriques réels peuvent diverger...         C1\n",
       "1   Le bleu, c'est ma couleur préférée mais je n'a...         A1\n",
       "2   Le test de niveau en français est sur le site ...         A1\n",
       "3            Est-ce que ton mari est aussi de Boston?         A1\n",
       "4   Dans les écoles de commerce, dans les couloirs...         B1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nous dûmes nous excuser des propos que nous eû...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vous ne pouvez pas savoir le plaisir que j'ai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Et, paradoxalement, boire froid n'est pas la b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ce n'est pas étonnant, car c'est une saison my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Le corps de Golo lui-même, d'une essence aussi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence\n",
       "id                                                   \n",
       "0   Nous dûmes nous excuser des propos que nous eû...\n",
       "1   Vous ne pouvez pas savoir le plaisir que j'ai ...\n",
       "2   Et, paradoxalement, boire froid n'est pas la b...\n",
       "3   Ce n'est pas étonnant, car c'est une saison my...\n",
       "4   Le corps de Golo lui-même, d'une essence aussi..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import the training data\n",
    "train = pd.read_csv('https://github.com/tcastrom/CEFR-French-/raw/main/Data/training_data.csv')\n",
    "train.set_index('id', inplace=True)\n",
    "display(train.head())\n",
    "\n",
    "#Import the unlabel data\n",
    "unlabel = pd.read_csv('https://github.com/tcastrom/CEFR-French-/raw/main/Data/unlabelled_test_data.csv')\n",
    "unlabel.set_index('id', inplace=True)\n",
    "display(unlabel.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A1', 'A2', 'B1', 'B2', 'C1', 'C2'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values [0, 1, 2, 3, 4, 5] are represented by['A1' 'A2' 'B1' 'B2' 'C1' 'C2']\n"
     ]
    }
   ],
   "source": [
    "# Encode the difficulty column using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train['difficulty'] = le.fit_transform(train['difficulty'])\n",
    "\n",
    "# Display how the difficulty has been encoded and the values associated with each encoding\n",
    "display(le.classes_)\n",
    "print(f'The values [0, 1, 2, 3, 4, 5] are represented by{le.inverse_transform([0, 1, 2, 3, 4, 5])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y\n",
    "X = train['sentence']\n",
    "y = train['difficulty']\n",
    "\n",
    "# Split the data into train and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the vectorizer on the training data\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the vectorizer on the testing data\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.26458333333333334\n",
      "Precision: 0.2502562030083607\n",
      "Recall: 0.26458333333333334\n",
      "F1 Score: 0.25270693757433055\n"
     ]
    }
   ],
   "source": [
    "# Do a linear regression \n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X_train, y_train)\n",
    "\n",
    "# Predict the difficulty on the testing data\n",
    "y_pred = linear_regression.predict(X_test)\n",
    "\n",
    "# Round the predictions to the nearest integer and make sure the values are between 0 and 5\n",
    "y_pred = np.round(y_pred)\n",
    "y_pred[y_pred < 0] = 0\n",
    "y_pred[y_pred > 5] = 5\n",
    "\n",
    "# Reverse the encoding of the difficulty\n",
    "y_test_unencoded = le.inverse_transform(y_test)\n",
    "y_pred_unencoded = le.inverse_transform(y_pred.astype(int))\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_reg_lin = accuracy_score(y_test_unencoded, y_pred_unencoded)\n",
    "print(f'Accuracy: {accuracy_reg_lin}')\n",
    "\n",
    "# Calculate the Precision \n",
    "precision_reg_lin = precision_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Precision: {precision_reg_lin}')\n",
    "\n",
    "# Calculate the Recall\n",
    "recall_reg_lin = recall_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Recall: {recall_reg_lin}')\n",
    "\n",
    "# Calculate the F1 Score\n",
    "f1_reg_lin = f1_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'F1 Score: {f1_reg_lin}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic  Regression with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.471875\n",
      "Precision: 0.4676454066489516\n",
      "Recall: 0.471875\n",
      "F1 Score: 0.4673314414307995\n"
     ]
    }
   ],
   "source": [
    "# Do a logistic regression woth cross validation\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# Create a logistic regression object\n",
    "logistic_regression = LogisticRegressionCV(cv=5, max_iter=1000)\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "# Predict the difficulty on the testing data\n",
    "y_pred = logistic_regression.predict(X_test)\n",
    "\n",
    "# Reverse the encoding of the difficulty\n",
    "y_test_unencoded = le.inverse_transform(y_test)\n",
    "y_pred_unencoded = le.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_log_reg = accuracy_score(y_test_unencoded, y_pred_unencoded)\n",
    "print(f'Accuracy: {accuracy_log_reg}')\n",
    "\n",
    "# Calculate the Precision\n",
    "precision_log_reg = precision_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Precision: {precision_log_reg}')\n",
    "\n",
    "# Calculate the Recall\n",
    "recall_log_reg = recall_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Recall: {recall_log_reg}')\n",
    "\n",
    "# Calculate the F1 Score\n",
    "f1_log_reg = f1_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'F1 Score: {f1_log_reg}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': range(1, 11),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search_knn = GridSearchCV(estimator=knn, param_grid=param_grid_knn, cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_knn = grid_search_knn.best_params_\n",
    "\n",
    "print(f\"The best parameters are : {best_params_knn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.35833333333333334\n",
      "Precision: 0.40422399185184915\n",
      "Recall: 0.35833333333333334\n",
      "F1 Score: 0.3464204117405712\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best model\n",
    "best_model_knn = grid_search_knn.best_estimator_\n",
    "\n",
    "# Predict the difficulty on the testing data\n",
    "y_pred = best_model_knn.predict(X_test)\n",
    "\n",
    "# Reverse the encoding of the difficulty\n",
    "y_test_unencoded = le.inverse_transform(y_test)\n",
    "y_pred_unencoded = le.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_knn = accuracy_score(y_test_unencoded, y_pred_unencoded)\n",
    "print(f'Accuracy: {accuracy_knn}')\n",
    "\n",
    "# Calculate the Precision\n",
    "precision_knn = precision_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Precision: {precision_knn}')\n",
    "\n",
    "# Calculate the Recall\n",
    "recall_knn = recall_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Recall: {recall_knn}')\n",
    "\n",
    "# Calculate the F1 Score\n",
    "f1_knn = f1_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'F1 Score: {f1_knn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "9600 fits failed out of a total of 28800.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "9600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : {'criterion': 'entropy', 'max_depth': 10, 'max_features': 'sqrt', 'max_leaf_nodes': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan ... 0.17916667 0.16822917 0.16901042]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree with GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_dt = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': range(1, 11),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_leaf_nodes': [None, 10, 20, 30]\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search_dt = GridSearchCV(estimator=dt, param_grid=param_grid_dt, cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_dt = grid_search_dt.best_params_\n",
    "\n",
    "print(f\"The best parameters are : {best_params_dt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.225\n",
      "Precision: 0.24208033550203492\n",
      "Recall: 0.225\n",
      "F1 Score: 0.1574943254423193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best model\n",
    "best_model_dt = grid_search_dt.best_estimator_\n",
    "\n",
    "# Predict the difficulty on the testing data\n",
    "y_pred = best_model_dt.predict(X_test)\n",
    "\n",
    "# Reverse the encoding of the difficulty\n",
    "y_test_unencoded = le.inverse_transform(y_test)\n",
    "y_pred_unencoded = le.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_dt = accuracy_score(y_test_unencoded, y_pred_unencoded)\n",
    "print(f'Accuracy: {accuracy_dt}')\n",
    "\n",
    "# Calculate the Precision\n",
    "precision_dt = precision_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Precision: {precision_dt}')\n",
    "\n",
    "# Calculate the Recall\n",
    "recall_dt = recall_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Recall: {recall_dt}')\n",
    "\n",
    "# Calculate the F1 Score\n",
    "f1_dt = f1_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'F1 Score: {f1_dt}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "3600 fits failed out of a total of 10800.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan ... 0.234375   0.240625   0.26588542]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : {'criterion': 'gini', 'max_depth': 9, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# Random Forest with GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': range(1, 11),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "print(f\"The best parameters are : {best_params_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.375\n",
      "Precision: 0.36757037874376164\n",
      "Recall: 0.375\n",
      "F1 Score: 0.3404847778939029\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best model\n",
    "best_model_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "# Predict the difficulty on the testing data\n",
    "y_pred = best_model_rf.predict(X_test)\n",
    "\n",
    "# Reverse the encoding of the difficulty\n",
    "y_test_unencoded = le.inverse_transform(y_test)\n",
    "y_pred_unencoded = le.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_rf = accuracy_score(y_test_unencoded, y_pred_unencoded)\n",
    "print(f'Accuracy: {accuracy_rf}')\n",
    "\n",
    "# Calculate the Precision\n",
    "precision_rf = precision_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Precision: {precision_rf}')\n",
    "\n",
    "# Calculate the Recall\n",
    "recall_rf = recall_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Recall: {recall_rf}')\n",
    "\n",
    "# Calculate the F1 Score\n",
    "f1_rf = f1_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'F1 Score: {f1_rf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suport Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : {'C': 10, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine with GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree': [2, 3, 4],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search_svm = GridSearchCV(estimator=svm, param_grid=param_grid_svm, cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_svm = grid_search_svm.best_params_\n",
    "print(f\"The best parameters are : {best_params_svm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4791666666666667\n",
      "Precision: 0.47571073217035076\n",
      "Recall: 0.4791666666666667\n",
      "F1 Score: 0.4760176557790857\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best model\n",
    "best_model_svm = grid_search_svm.best_estimator_\n",
    "\n",
    "# Predict the difficulty on the testing data\n",
    "y_pred = best_model_svm.predict(X_test)\n",
    "\n",
    "# Reverse the encoding of the difficulty\n",
    "y_test_unencoded = le.inverse_transform(y_test)\n",
    "y_pred_unencoded = le.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_svm = accuracy_score(y_test_unencoded, y_pred_unencoded)\n",
    "print(f'Accuracy: {accuracy_svm}')\n",
    "\n",
    "# Calculate the Precision\n",
    "precision_svm = precision_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Precision: {precision_svm}')\n",
    "\n",
    "# Calculate the Recall\n",
    "recall_svm = recall_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Recall: {recall_svm}')\n",
    "\n",
    "# Calculate the F1 Score\n",
    "f1_svm = f1_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'F1 Score: {f1_svm}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : {'alpha': 1, 'fit_prior': True}\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes with GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_nb = {\n",
    "    'alpha': [0.1, 1, 10],\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search_nb = GridSearchCV(estimator=nb, param_grid=param_grid_nb, cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_nb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_nb = grid_search_nb.best_params_\n",
    "print(f\"The best parameters are : {best_params_nb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.45416666666666666\n",
      "Precision: 0.4689134938302318\n",
      "Recall: 0.45416666666666666\n",
      "F1 Score: 0.4523446042056213\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best model\n",
    "best_model_nb = grid_search_nb.best_estimator_\n",
    "\n",
    "# Predict the difficulty on the testing data\n",
    "y_pred = best_model_nb.predict(X_test)\n",
    "\n",
    "# Reverse the encoding of the difficulty\n",
    "y_test_unencoded = le.inverse_transform(y_test)\n",
    "y_pred_unencoded = le.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_nb = accuracy_score(y_test_unencoded, y_pred_unencoded)\n",
    "print(f'Accuracy: {accuracy_nb}')\n",
    "\n",
    "# Calculate the Precision\n",
    "precision_nb = precision_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Precision: {precision_nb}')\n",
    "\n",
    "# Calculate the Recall\n",
    "recall_nb = recall_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Recall: {recall_nb}')\n",
    "\n",
    "# Calculate the F1 Score\n",
    "f1_nb = f1_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'F1 Score: {f1_nb}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : {'alpha': 0.001, 'loss': 'squared_hinge', 'max_iter': 5000, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "180 fits failed out of a total of 900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'huber', 'hinge', 'perceptron', 'log_loss', 'squared_hinge', 'squared_error', 'modified_huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'}. Got 'log' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.434375   0.42005208 0.4375     0.43125    0.41875    0.43098958\n",
      " 0.43098958 0.42369792 0.43776042        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.43151042 0.403125   0.42578125 0.43177083 0.39401042 0.43203125\n",
      " 0.42708333 0.40130208 0.42578125 0.30859375 0.3609375  0.31328125\n",
      " 0.3125     0.35963542 0.32786458 0.32421875 0.36197917 0.32864583\n",
      " 0.40729167 0.32369792 0.3953125  0.40677083 0.34088542 0.39791667\n",
      " 0.4125     0.32317708 0.39869792 0.43671875 0.23020833 0.38697917\n",
      " 0.43723958 0.25052083 0.39244792 0.43697917 0.2421875  0.3921875\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.44895833 0.3765625  0.43463542\n",
      " 0.4484375  0.37317708 0.43541667 0.4484375  0.37734375 0.434375\n",
      " 0.44791667 0.35963542 0.43854167 0.4484375  0.35677083 0.43333333\n",
      " 0.45104167 0.35885417 0.43619792 0.40885417 0.19713542 0.28020833\n",
      " 0.41197917 0.20572917 0.29895833 0.403125   0.19270833 0.28515625\n",
      " 0.43567708 0.1671875  0.20729167 0.43229167 0.16536458 0.19192708\n",
      " 0.4359375  0.16666667 0.1890625         nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.4234375  0.25755208 0.34244792 0.42135417 0.26588542 0.34192708\n",
      " 0.42213542 0.25807292 0.34375    0.42161458 0.26197917 0.34739583\n",
      " 0.42161458 0.26145833 0.34088542 0.42265625 0.26328125 0.34166667\n",
      " 0.41536458 0.1671875  0.1921875  0.41145833 0.16510417 0.19348958\n",
      " 0.40651042 0.16666667 0.18697917 0.225      0.16692708 0.2078125\n",
      " 0.24739583 0.16666667 0.20234375 0.24010417 0.16640625 0.18229167\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.3703125  0.16770833 0.21744792\n",
      " 0.36119792 0.16822917 0.22213542 0.36432292 0.1671875  0.22682292\n",
      " 0.36536458 0.1671875  0.25052083 0.371875   0.16770833 0.23072917\n",
      " 0.37083333 0.16770833 0.22447917 0.40833333 0.1671875  0.1671875\n",
      " 0.41015625 0.16744792 0.16614583 0.40520833 0.16484375 0.165625  ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent with GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_sgd = {\n",
    "    'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'max_iter': [1000, 2000, 5000]\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "sgd = SGDClassifier()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search_sgd = GridSearchCV(estimator=sgd, param_grid=param_grid_sgd, cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_sgd.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_sgd = grid_search_sgd.best_params_\n",
    "print(f\"The best parameters are : {best_params_sgd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.46145833333333336\n",
      "Precision: 0.45711469335565347\n",
      "Recall: 0.46145833333333336\n",
      "F1 Score: 0.4523827674052193\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best model\n",
    "best_model_sgd = grid_search_sgd.best_estimator_\n",
    "\n",
    "# Predict the difficulty on the testing data\n",
    "y_pred = best_model_sgd.predict(X_test)\n",
    "\n",
    "# Reverse the encoding of the difficulty\n",
    "y_test_unencoded = le.inverse_transform(y_test)\n",
    "y_pred_unencoded = le.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_sgd = accuracy_score(y_test_unencoded, y_pred_unencoded)\n",
    "print(f'Accuracy: {accuracy_sgd}')\n",
    "\n",
    "# Calculate the Precision\n",
    "precision_sgd = precision_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Precision: {precision_sgd}')\n",
    "\n",
    "# Calculate the Recall\n",
    "recall_sgd = recall_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Recall: {recall_sgd}')\n",
    "\n",
    "# Calculate the F1 Score\n",
    "f1_sgd = f1_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'F1 Score: {f1_sgd}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machines (GBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1350 fits failed out of a total of 4050.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1350 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.30729167 0.32057292 0.33515625\n",
      " 0.35364583 0.3546875  0.35911458 0.38307292 0.38177083 0.3921875\n",
      " 0.24791667 0.22057292 0.25052083 0.27786458 0.28046875 0.28958333\n",
      " 0.32291667 0.32083333 0.32942708        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.35364583 0.35494792 0.3578125  0.37604167 0.39010417 0.39375\n",
      " 0.40260417 0.40598958 0.41354167 0.26432292 0.30520833 0.303125\n",
      " 0.29166667 0.31953125 0.32916667 0.33854167 0.35208333 0.359375\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.36536458 0.37890625 0.38125\n",
      " 0.38932292 0.39765625 0.40104167 0.4140625  0.40989583 0.41588542\n",
      " 0.28776042 0.30572917 0.30625    0.32890625 0.33541667 0.34192708\n",
      " 0.37213542 0.37109375 0.384375          nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.36744792 0.38125    0.38932292 0.41015625 0.39921875 0.41067708\n",
      " 0.43255208 0.43177083 0.42864583 0.32161458 0.32578125 0.32109375\n",
      " 0.3390625  0.34479167 0.36067708 0.3734375  0.40703125 0.40729167\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.39244792 0.40364583 0.39817708\n",
      " 0.40833333 0.42135417 0.41536458 0.42994792 0.43776042 0.43541667\n",
      " 0.30598958 0.33151042 0.32916667 0.34973958 0.36302083 0.37395833\n",
      " 0.39375    0.39609375 0.40546875        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.38958333 0.40364583 0.40286458 0.4171875  0.42005208 0.42135417\n",
      " 0.43567708 0.44088542 0.4375     0.32552083 0.32838542 0.33802083\n",
      " 0.34817708 0.36927083 0.38697917 0.40234375 0.42109375 0.41692708\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.41119792 0.40052083 0.41015625\n",
      " 0.42291667 0.42942708 0.43177083 0.4375     0.44010417 0.44713542\n",
      " 0.32161458 0.35546875 0.35260417 0.3734375  0.38515625 0.40078125\n",
      " 0.39765625 0.40208333 0.41848958        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.41015625 0.41328125 0.41536458 0.43463542 0.44166667 0.43958333\n",
      " 0.45052083 0.44114583 0.44635417 0.33984375 0.35286458 0.35729167\n",
      " 0.37161458 0.40729167 0.39375    0.40833333 0.421875   0.41067708\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.4015625  0.4171875  0.42057292\n",
      " 0.43463542 0.4296875  0.43333333 0.44296875 0.44557292 0.44947917\n",
      " 0.34869792 0.36171875 0.36744792 0.38229167 0.38958333 0.39973958\n",
      " 0.40364583 0.434375   0.44010417        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.41484375 0.4171875  0.42135417 0.4359375  0.43776042 0.43307292\n",
      " 0.44661458 0.4578125  0.45234375 0.36640625 0.36119792 0.36953125\n",
      " 0.378125   0.38828125 0.4078125  0.41692708 0.42291667 0.44296875\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.33177083 0.33880208 0.33411458\n",
      " 0.36484375 0.36536458 0.378125   0.39453125 0.40078125 0.39739583\n",
      " 0.25       0.26666667 0.23567708 0.29348958 0.29765625 0.290625\n",
      " 0.32578125 0.3296875  0.32994792        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.35       0.36536458 0.38203125 0.39375    0.40807292 0.40442708\n",
      " 0.41901042 0.4265625  0.43151042 0.27630208 0.3015625  0.29739583\n",
      " 0.31848958 0.32786458 0.34244792 0.35572917 0.35807292 0.37083333\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.37734375 0.38541667 0.39296875\n",
      " 0.41197917 0.42291667 0.42994792 0.43984375 0.43151042 0.45130208\n",
      " 0.29973958 0.30963542 0.32942708 0.34348958 0.33854167 0.35078125\n",
      " 0.35963542 0.37682292 0.39322917        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.39583333 0.3953125  0.4046875  0.40208333 0.44010417 0.43359375\n",
      " 0.44817708 0.45598958 0.4453125  0.32473958 0.3203125  0.35052083\n",
      " 0.34557292 0.35703125 0.36796875 0.3875     0.39479167 0.39739583\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.39973958 0.39348958 0.41953125\n",
      " 0.42760417 0.42864583 0.44140625 0.44635417 0.44713542 0.44713542\n",
      " 0.32760417 0.33671875 0.34765625 0.35234375 0.3640625  0.38072917\n",
      " 0.37265625 0.41354167 0.41588542        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.40104167 0.415625   0.42161458 0.42473958 0.42447917 0.44192708\n",
      " 0.44739583 0.4515625  0.44427083 0.33489583 0.3375     0.36796875\n",
      " 0.35859375 0.37552083 0.39869792 0.39817708 0.41848958 0.44088542\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.41119792 0.41953125 0.42552083\n",
      " 0.44140625 0.44322917 0.44088542 0.43958333 0.45286458 0.45494792\n",
      " 0.33697917 0.3484375  0.35677083 0.37135417 0.39114583 0.39270833\n",
      " 0.41171875 0.421875   0.41875           nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.42786458 0.43515625 0.42916667 0.446875   0.45130208 0.44375\n",
      " 0.45104167 0.4578125  0.45026042 0.34557292 0.36770833 0.36067708\n",
      " 0.37447917 0.39453125 0.409375   0.4328125  0.41197917 0.43177083\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.41354167 0.43854167 0.43411458\n",
      " 0.43307292 0.43932292 0.44140625 0.45338542 0.45390625 0.45755208\n",
      " 0.34505208 0.38697917 0.36614583 0.4        0.38489583 0.39479167\n",
      " 0.4234375  0.42109375 0.43333333        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.42630208 0.43463542 0.434375   0.43229167 0.45104167 0.44817708\n",
      " 0.45       0.45807292 0.44947917 0.35651042 0.36927083 0.38072917\n",
      " 0.39453125 0.39348958 0.40833333 0.42552083 0.44088542 0.44166667\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.31848958 0.33411458 0.33411458\n",
      " 0.33229167 0.36510417 0.36848958 0.34557292 0.35729167 0.37916667\n",
      " 0.26953125 0.25885417 0.24947917 0.29166667 0.29947917 0.29947917\n",
      " 0.30651042 0.32421875 0.31640625        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.3296875  0.353125   0.36458333 0.36614583 0.36901042 0.3765625\n",
      " 0.34661458 0.36380208 0.39557292 0.29427083 0.28333333 0.30026042\n",
      " 0.29244792 0.32239583 0.33020833 0.32265625 0.33229167 0.35546875\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.34140625 0.36067708 0.37994792\n",
      " 0.3484375  0.36979167 0.38385417 0.35052083 0.371875   0.40026042\n",
      " 0.29713542 0.29401042 0.309375   0.31848958 0.32083333 0.32578125\n",
      " 0.32291667 0.33671875 0.34401042        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.33932292 0.36979167 0.37604167 0.35703125 0.38072917 0.38567708\n",
      " 0.35572917 0.36484375 0.38541667 0.30286458 0.30677083 0.3203125\n",
      " 0.30703125 0.32942708 0.33932292 0.32604167 0.34713542 0.35182292\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.34036458 0.36744792 0.38828125\n",
      " 0.34947917 0.37109375 0.39401042 0.3453125  0.35598958 0.36484375\n",
      " 0.30104167 0.32916667 0.33098958 0.31067708 0.33307292 0.35234375\n",
      " 0.32421875 0.3484375  0.35729167        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.34973958 0.37317708 0.38541667 0.34557292 0.35963542 0.3859375\n",
      " 0.3375     0.3421875  0.38125    0.30234375 0.31354167 0.3359375\n",
      " 0.30911458 0.34140625 0.32734375 0.31536458 0.33828125 0.35651042\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.34661458 0.36692708 0.378125\n",
      " 0.35208333 0.37291667 0.40546875 0.32473958 0.33880208 0.3578125\n",
      " 0.31901042 0.32760417 0.34401042 0.31796875 0.33880208 0.35989583\n",
      " 0.3203125  0.34140625 0.36536458        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.34635417 0.36796875 0.36796875 0.33229167 0.35598958 0.39739583\n",
      " 0.34010417 0.33411458 0.37526042 0.30338542 0.33125    0.31484375\n",
      " 0.30755208 0.33333333 0.34713542 0.31666667 0.3359375  0.38098958\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.35755208 0.36536458 0.38046875\n",
      " 0.33515625 0.34947917 0.39505208 0.32083333 0.32473958 0.38776042\n",
      " 0.31666667 0.3421875  0.33958333 0.32239583 0.33541667 0.35078125\n",
      " 0.290625   0.32994792 0.38411458        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.34088542 0.36614583 0.38697917 0.33645833 0.35703125 0.37578125\n",
      " 0.31223958 0.3046875  0.37552083 0.32135417 0.32604167 0.33385417\n",
      " 0.32890625 0.33828125 0.35442708 0.2953125  0.33567708 0.32682292]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : {'learning_rate': 0.1, 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 200, 'subsample': 0.75}\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting with GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_depth': range(1, 11),\n",
    "    'subsample': [0.5, 0.75, 1],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search_gb = GridSearchCV(estimator=gb, param_grid=param_grid_gb, cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_gb = grid_search_gb.best_params_\n",
    "print(f\"The best parameters are : {best_params_gb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4395833333333333\n",
      "Precision: 0.436979592928103\n",
      "Recall: 0.4395833333333333\n",
      "F1 Score: 0.4342079023645971\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best model\n",
    "best_model_gb = grid_search_gb.best_estimator_\n",
    "\n",
    "# Predict the difficulty on the testing data\n",
    "y_pred = best_model_gb.predict(X_test)\n",
    "\n",
    "# Reverse the encoding of the difficulty\n",
    "y_test_unencoded = le.inverse_transform(y_test)\n",
    "y_pred_unencoded = le.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_gb = accuracy_score(y_test_unencoded, y_pred_unencoded)\n",
    "print(f'Accuracy: {accuracy_gb}')\n",
    "\n",
    "# Calculate the Precision\n",
    "precision_gb = precision_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Precision: {precision_gb}')\n",
    "\n",
    "# Calculate the Recall\n",
    "recall_gb = recall_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Recall: {recall_gb}')\n",
    "\n",
    "# Calculate the F1 Score\n",
    "f1_gb = f1_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'F1 Score: {f1_gb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : {'algorithm': 'SAMME.R', 'learning_rate': 1, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost with GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_ab = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "ab = AdaBoostClassifier()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search_ab = GridSearchCV(estimator=ab, param_grid=param_grid_ab, cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_ab.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_ab = grid_search_ab.best_params_\n",
    "print(f\"The best parameters are : {best_params_ab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.35104166666666664\n",
      "Precision: 0.3556328245235454\n",
      "Recall: 0.35104166666666664\n",
      "F1 Score: 0.34769699642321156\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best model\n",
    "best_model_ab = grid_search_ab.best_estimator_\n",
    "\n",
    "# Predict the difficulty on the testing data\n",
    "y_pred = best_model_ab.predict(X_test)\n",
    "\n",
    "# Reverse the encoding of the difficulty\n",
    "y_test_unencoded = le.inverse_transform(y_test)\n",
    "y_pred_unencoded = le.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_ab = accuracy_score(y_test_unencoded, y_pred_unencoded)\n",
    "print(f'Accuracy: {accuracy_ab}')\n",
    "\n",
    "# Calculate the Precision\n",
    "precision_ab = precision_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Precision: {precision_ab}')\n",
    "\n",
    "# Calculate the Recall\n",
    "recall_ab = recall_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Recall: {recall_ab}')\n",
    "\n",
    "# Calculate the F1 Score\n",
    "f1_ab = f1_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'F1 Score: {f1_ab}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic Discriminant Analysis (QDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\castr\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : {'reg_param': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Quadratic Discriminant Analysis with GridSearchCV\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_qda = {\n",
    "    'reg_param': [0.0, 0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search_qda = GridSearchCV(estimator=qda, param_grid=param_grid_qda, cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_qda.fit(X_train.toarray(), y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_qda = grid_search_qda.best_params_\n",
    "print(f\"The best parameters are : {best_params_qda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.17395833333333333\n",
      "Precision: 0.20042869728471632\n",
      "Recall: 0.17395833333333333\n",
      "F1 Score: 0.1506847015702394\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best model\n",
    "best_model_qda = grid_search_qda.best_estimator_\n",
    "\n",
    "# Predict the difficulty on the testing data\n",
    "y_pred = best_model_qda.predict(X_test.toarray())\n",
    "\n",
    "# Reverse the encoding of the difficulty\n",
    "y_test_unencoded = le.inverse_transform(y_test)\n",
    "y_pred_unencoded = le.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_qda = accuracy_score(y_test_unencoded, y_pred_unencoded)\n",
    "print(f'Accuracy: {accuracy_qda}')\n",
    "\n",
    "# Calculate the Precision\n",
    "precision_qda = precision_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Precision: {precision_qda}')\n",
    "\n",
    "# Calculate the Recall\n",
    "recall_qda = recall_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'Recall: {recall_qda}')\n",
    "\n",
    "# Calculate the F1 Score\n",
    "f1_qda = f1_score(y_test_unencoded, y_pred_unencoded, average='weighted')\n",
    "print(f'F1 Score: {f1_qda}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Quadratic Discriminant Analysis</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Model  Accuracy  Precision  Recall  F1 Score\n",
       "0                 Linear Regression      0.26       0.25    0.26      0.25\n",
       "1               Logistic Regression      0.47       0.47    0.47      0.47\n",
       "2               K-Nearest Neighbors      0.36       0.40    0.36      0.35\n",
       "3                     Decision Tree      0.22       0.24    0.22      0.16\n",
       "4                     Random Forest      0.38       0.37    0.38      0.34\n",
       "5            Support Vector Machine      0.48       0.48    0.48      0.48\n",
       "6                       Naive Bayes      0.45       0.47    0.45      0.45\n",
       "7       Stochastic Gradient Descent      0.46       0.46    0.46      0.45\n",
       "8                 Gradient Boosting      0.44       0.44    0.44      0.43\n",
       "9                          AdaBoost      0.35       0.36    0.35      0.35\n",
       "10  Quadratic Discriminant Analysis      0.17       0.20    0.17      0.15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\castr\\AppData\\Local\\Temp\\ipykernel_25852\\1562804461.py:25: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  formatted_results = results.applymap(format_sigfig)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f4a83 thead th {\n",
       "  background-color: grey;\n",
       "  color: black;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_f4a83 tbody tr:nth-child(even) {\n",
       "  background-color: #f2f2f2;\n",
       "  color: black;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_f4a83 tbody tr:nth-child(odd) {\n",
       "  background-color: white;\n",
       "  color: black;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_f4a83_row0_col0, #T_f4a83_row0_col1, #T_f4a83_row0_col2, #T_f4a83_row0_col3, #T_f4a83_row0_col4, #T_f4a83_row1_col0, #T_f4a83_row1_col1, #T_f4a83_row1_col2, #T_f4a83_row1_col3, #T_f4a83_row1_col4, #T_f4a83_row2_col0, #T_f4a83_row2_col1, #T_f4a83_row2_col2, #T_f4a83_row2_col3, #T_f4a83_row2_col4, #T_f4a83_row3_col0, #T_f4a83_row3_col1, #T_f4a83_row3_col2, #T_f4a83_row3_col3, #T_f4a83_row3_col4, #T_f4a83_row4_col0, #T_f4a83_row4_col1, #T_f4a83_row4_col2, #T_f4a83_row4_col3, #T_f4a83_row4_col4, #T_f4a83_row5_col0, #T_f4a83_row5_col1, #T_f4a83_row5_col2, #T_f4a83_row5_col3, #T_f4a83_row5_col4, #T_f4a83_row6_col0, #T_f4a83_row6_col1, #T_f4a83_row6_col2, #T_f4a83_row6_col3, #T_f4a83_row6_col4, #T_f4a83_row7_col0, #T_f4a83_row7_col1, #T_f4a83_row7_col2, #T_f4a83_row7_col3, #T_f4a83_row7_col4, #T_f4a83_row8_col0, #T_f4a83_row8_col1, #T_f4a83_row8_col2, #T_f4a83_row8_col3, #T_f4a83_row8_col4, #T_f4a83_row9_col0, #T_f4a83_row9_col1, #T_f4a83_row9_col2, #T_f4a83_row9_col3, #T_f4a83_row9_col4, #T_f4a83_row10_col0, #T_f4a83_row10_col1, #T_f4a83_row10_col2, #T_f4a83_row10_col3, #T_f4a83_row10_col4 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f4a83\">\n",
       "  <caption>Model Performance Comparison</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f4a83_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_f4a83_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_f4a83_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_f4a83_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_f4a83_level0_col4\" class=\"col_heading level0 col4\" >F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f4a83_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f4a83_row0_col0\" class=\"data row0 col0\" >Linear Regression</td>\n",
       "      <td id=\"T_f4a83_row0_col1\" class=\"data row0 col1\" >0.26</td>\n",
       "      <td id=\"T_f4a83_row0_col2\" class=\"data row0 col2\" >0.25</td>\n",
       "      <td id=\"T_f4a83_row0_col3\" class=\"data row0 col3\" >0.26</td>\n",
       "      <td id=\"T_f4a83_row0_col4\" class=\"data row0 col4\" >0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4a83_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f4a83_row1_col0\" class=\"data row1 col0\" >Logistic Regression</td>\n",
       "      <td id=\"T_f4a83_row1_col1\" class=\"data row1 col1\" >0.47</td>\n",
       "      <td id=\"T_f4a83_row1_col2\" class=\"data row1 col2\" >0.47</td>\n",
       "      <td id=\"T_f4a83_row1_col3\" class=\"data row1 col3\" >0.47</td>\n",
       "      <td id=\"T_f4a83_row1_col4\" class=\"data row1 col4\" >0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4a83_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f4a83_row2_col0\" class=\"data row2 col0\" >K-Nearest Neighbors</td>\n",
       "      <td id=\"T_f4a83_row2_col1\" class=\"data row2 col1\" >0.36</td>\n",
       "      <td id=\"T_f4a83_row2_col2\" class=\"data row2 col2\" >0.4</td>\n",
       "      <td id=\"T_f4a83_row2_col3\" class=\"data row2 col3\" >0.36</td>\n",
       "      <td id=\"T_f4a83_row2_col4\" class=\"data row2 col4\" >0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4a83_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f4a83_row3_col0\" class=\"data row3 col0\" >Decision Tree</td>\n",
       "      <td id=\"T_f4a83_row3_col1\" class=\"data row3 col1\" >0.22</td>\n",
       "      <td id=\"T_f4a83_row3_col2\" class=\"data row3 col2\" >0.24</td>\n",
       "      <td id=\"T_f4a83_row3_col3\" class=\"data row3 col3\" >0.22</td>\n",
       "      <td id=\"T_f4a83_row3_col4\" class=\"data row3 col4\" >0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4a83_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f4a83_row4_col0\" class=\"data row4 col0\" >Random Forest</td>\n",
       "      <td id=\"T_f4a83_row4_col1\" class=\"data row4 col1\" >0.38</td>\n",
       "      <td id=\"T_f4a83_row4_col2\" class=\"data row4 col2\" >0.37</td>\n",
       "      <td id=\"T_f4a83_row4_col3\" class=\"data row4 col3\" >0.38</td>\n",
       "      <td id=\"T_f4a83_row4_col4\" class=\"data row4 col4\" >0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4a83_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_f4a83_row5_col0\" class=\"data row5 col0\" >Support Vector Machine</td>\n",
       "      <td id=\"T_f4a83_row5_col1\" class=\"data row5 col1\" >0.48</td>\n",
       "      <td id=\"T_f4a83_row5_col2\" class=\"data row5 col2\" >0.48</td>\n",
       "      <td id=\"T_f4a83_row5_col3\" class=\"data row5 col3\" >0.48</td>\n",
       "      <td id=\"T_f4a83_row5_col4\" class=\"data row5 col4\" >0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4a83_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_f4a83_row6_col0\" class=\"data row6 col0\" >Naive Bayes</td>\n",
       "      <td id=\"T_f4a83_row6_col1\" class=\"data row6 col1\" >0.45</td>\n",
       "      <td id=\"T_f4a83_row6_col2\" class=\"data row6 col2\" >0.47</td>\n",
       "      <td id=\"T_f4a83_row6_col3\" class=\"data row6 col3\" >0.45</td>\n",
       "      <td id=\"T_f4a83_row6_col4\" class=\"data row6 col4\" >0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4a83_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_f4a83_row7_col0\" class=\"data row7 col0\" >Stochastic Gradient Descent</td>\n",
       "      <td id=\"T_f4a83_row7_col1\" class=\"data row7 col1\" >0.46</td>\n",
       "      <td id=\"T_f4a83_row7_col2\" class=\"data row7 col2\" >0.46</td>\n",
       "      <td id=\"T_f4a83_row7_col3\" class=\"data row7 col3\" >0.46</td>\n",
       "      <td id=\"T_f4a83_row7_col4\" class=\"data row7 col4\" >0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4a83_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_f4a83_row8_col0\" class=\"data row8 col0\" >Gradient Boosting</td>\n",
       "      <td id=\"T_f4a83_row8_col1\" class=\"data row8 col1\" >0.44</td>\n",
       "      <td id=\"T_f4a83_row8_col2\" class=\"data row8 col2\" >0.44</td>\n",
       "      <td id=\"T_f4a83_row8_col3\" class=\"data row8 col3\" >0.44</td>\n",
       "      <td id=\"T_f4a83_row8_col4\" class=\"data row8 col4\" >0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4a83_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_f4a83_row9_col0\" class=\"data row9 col0\" >AdaBoost</td>\n",
       "      <td id=\"T_f4a83_row9_col1\" class=\"data row9 col1\" >0.35</td>\n",
       "      <td id=\"T_f4a83_row9_col2\" class=\"data row9 col2\" >0.36</td>\n",
       "      <td id=\"T_f4a83_row9_col3\" class=\"data row9 col3\" >0.35</td>\n",
       "      <td id=\"T_f4a83_row9_col4\" class=\"data row9 col4\" >0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4a83_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_f4a83_row10_col0\" class=\"data row10 col0\" >Quadratic Discriminant Analysis</td>\n",
       "      <td id=\"T_f4a83_row10_col1\" class=\"data row10 col1\" >0.17</td>\n",
       "      <td id=\"T_f4a83_row10_col2\" class=\"data row10 col2\" >0.2</td>\n",
       "      <td id=\"T_f4a83_row10_col3\" class=\"data row10 col3\" >0.17</td>\n",
       "      <td id=\"T_f4a83_row10_col4\" class=\"data row10 col4\" >0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2a20523c3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compile all the results into a Table. The columns are : Model, Accuracy, Precision, Recall, F1 Score \n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Logistic Regression', 'K-Nearest Neighbors', 'Decision Tree', 'Random Forest', 'Support Vector Machine', 'Naive Bayes', 'Stochastic Gradient Descent', 'Gradient Boosting', 'AdaBoost', 'Quadratic Discriminant Analysis'],\n",
    "    'Accuracy': [accuracy_reg_lin, accuracy_log_reg, accuracy_knn, accuracy_dt, accuracy_rf, accuracy_svm, accuracy_nb, accuracy_sgd, accuracy_gb, accuracy_ab, accuracy_qda],\n",
    "    'Precision': [precision_reg_lin, precision_log_reg, precision_knn, precision_dt, precision_rf, precision_svm, precision_nb, precision_sgd, precision_gb, precision_ab, precision_qda],\n",
    "    'Recall': [recall_reg_lin, recall_log_reg, recall_knn, recall_dt, recall_rf, recall_svm, recall_nb, recall_sgd, recall_gb, recall_ab, recall_qda],\n",
    "    'F1 Score': [f1_reg_lin, f1_log_reg, f1_knn, f1_dt, f1_rf, f1_svm, f1_nb, f1_sgd, f1_gb, f1_ab, f1_qda]\n",
    "})\n",
    "\n",
    "#Round the values to 2 decimal places\n",
    "results = results.round(2)\n",
    "\n",
    "display(results)\n",
    "\n",
    "# Function to format numbers to 2 significant figures\n",
    "def format_sigfig(x):\n",
    "    if isinstance(x, float):\n",
    "        return '{:.2g}'.format(x)\n",
    "    return x\n",
    "\n",
    "# Apply formatting function to the DataFrame\n",
    "formatted_results = results.applymap(format_sigfig)\n",
    "\n",
    "\n",
    "# Use the .style attribute for better formatting\n",
    "styled_results = formatted_results.style.set_table_styles([\n",
    "    {'selector': 'thead th', 'props': [('background-color', 'grey'), ('color', 'black'), ('font-weight', 'bold')]},\n",
    "    {'selector': 'tbody tr:nth-child(even)', 'props': [('background-color', '#f2f2f2'),('color', 'black'), ('font-weight', 'bold')]},\n",
    "    {'selector': 'tbody tr:nth-child(odd)', 'props': [('background-color', 'white'),('color', 'black'), ('font-weight', 'bold')]}\n",
    "]).set_properties(**{'text-align': 'center'}).set_caption(\"Model Performance Comparison\")\n",
    "\n",
    "display(styled_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "#T_f4a83 thead th {\n",
    "  background-color: grey;\n",
    "  color: black;\n",
    "  font-weight: bold;\n",
    "}\n",
    "#T_f4a83 tbody tr:nth-child(even) {\n",
    "  background-color: #f2f2f2;\n",
    "  color: black;\n",
    "  font-weight: bold;\n",
    "}\n",
    "#T_f4a83 tbody tr:nth-child(odd) {\n",
    "  background-color: white;\n",
    "  color: black;\n",
    "  font-weight: bold;\n",
    "}\n",
    "#T_f4a83_row0_col0, #T_f4a83_row0_col1, #T_f4a83_row0_col2, #T_f4a83_row0_col3, #T_f4a83_row0_col4, #T_f4a83_row1_col0, #T_f4a83_row1_col1, #T_f4a83_row1_col2, #T_f4a83_row1_col3, #T_f4a83_row1_col4, #T_f4a83_row2_col0, #T_f4a83_row2_col1, #T_f4a83_row2_col2, #T_f4a83_row2_col3, #T_f4a83_row2_col4, #T_f4a83_row3_col0, #T_f4a83_row3_col1, #T_f4a83_row3_col2, #T_f4a83_row3_col3, #T_f4a83_row3_col4, #T_f4a83_row4_col0, #T_f4a83_row4_col1, #T_f4a83_row4_col2, #T_f4a83_row4_col3, #T_f4a83_row4_col4, #T_f4a83_row5_col0, #T_f4a83_row5_col1, #T_f4a83_row5_col2, #T_f4a83_row5_col3, #T_f4a83_row5_col4, #T_f4a83_row6_col0, #T_f4a83_row6_col1, #T_f4a83_row6_col2, #T_f4a83_row6_col3, #T_f4a83_row6_col4, #T_f4a83_row7_col0, #T_f4a83_row7_col1, #T_f4a83_row7_col2, #T_f4a83_row7_col3, #T_f4a83_row7_col4, #T_f4a83_row8_col0, #T_f4a83_row8_col1, #T_f4a83_row8_col2, #T_f4a83_row8_col3, #T_f4a83_row8_col4, #T_f4a83_row9_col0, #T_f4a83_row9_col1, #T_f4a83_row9_col2, #T_f4a83_row9_col3, #T_f4a83_row9_col4, #T_f4a83_row10_col0, #T_f4a83_row10_col1, #T_f4a83_row10_col2, #T_f4a83_row10_col3, #T_f4a83_row10_col4 {\n",
    "  text-align: center;\n",
    "}\n",
    "</style>\n",
    "<table id=\"T_f4a83\">\n",
    "  <caption>Model Performance Comparison</caption>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th class=\"blank level0\" >&nbsp;</th>\n",
    "      <th id=\"T_f4a83_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
    "      <th id=\"T_f4a83_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
    "      <th id=\"T_f4a83_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
    "      <th id=\"T_f4a83_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
    "      <th id=\"T_f4a83_level0_col4\" class=\"col_heading level0 col4\" >F1 Score</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th id=\"T_f4a83_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
    "      <td id=\"T_f4a83_row0_col0\" class=\"data row0 col0\" >Linear Regression</td>\n",
    "      <td id=\"T_f4a83_row0_col1\" class=\"data row0 col1\" >0.26</td>\n",
    "      <td id=\"T_f4a83_row0_col2\" class=\"data row0 col2\" >0.25</td>\n",
    "      <td id=\"T_f4a83_row0_col3\" class=\"data row0 col3\" >0.26</td>\n",
    "      <td id=\"T_f4a83_row0_col4\" class=\"data row0 col4\" >0.25</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_f4a83_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
    "      <td id=\"T_f4a83_row1_col0\" class=\"data row1 col0\" >Logistic Regression</td>\n",
    "      <td id=\"T_f4a83_row1_col1\" class=\"data row1 col1\" >0.47</td>\n",
    "      <td id=\"T_f4a83_row1_col2\" class=\"data row1 col2\" >0.47</td>\n",
    "      <td id=\"T_f4a83_row1_col3\" class=\"data row1 col3\" >0.47</td>\n",
    "      <td id=\"T_f4a83_row1_col4\" class=\"data row1 col4\" >0.47</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_f4a83_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
    "      <td id=\"T_f4a83_row2_col0\" class=\"data row2 col0\" >K-Nearest Neighbors</td>\n",
    "      <td id=\"T_f4a83_row2_col1\" class=\"data row2 col1\" >0.36</td>\n",
    "      <td id=\"T_f4a83_row2_col2\" class=\"data row2 col2\" >0.4</td>\n",
    "      <td id=\"T_f4a83_row2_col3\" class=\"data row2 col3\" >0.36</td>\n",
    "      <td id=\"T_f4a83_row2_col4\" class=\"data row2 col4\" >0.35</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_f4a83_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
    "      <td id=\"T_f4a83_row3_col0\" class=\"data row3 col0\" >Decision Tree</td>\n",
    "      <td id=\"T_f4a83_row3_col1\" class=\"data row3 col1\" >0.22</td>\n",
    "      <td id=\"T_f4a83_row3_col2\" class=\"data row3 col2\" >0.24</td>\n",
    "      <td id=\"T_f4a83_row3_col3\" class=\"data row3 col3\" >0.22</td>\n",
    "      <td id=\"T_f4a83_row3_col4\" class=\"data row3 col4\" >0.16</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_f4a83_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
    "      <td id=\"T_f4a83_row4_col0\" class=\"data row4 col0\" >Random Forest</td>\n",
    "      <td id=\"T_f4a83_row4_col1\" class=\"data row4 col1\" >0.38</td>\n",
    "      <td id=\"T_f4a83_row4_col2\" class=\"data row4 col2\" >0.37</td>\n",
    "      <td id=\"T_f4a83_row4_col3\" class=\"data row4 col3\" >0.38</td>\n",
    "      <td id=\"T_f4a83_row4_col4\" class=\"data row4 col4\" >0.34</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_f4a83_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
    "      <td id=\"T_f4a83_row5_col0\" class=\"data row5 col0\" >Support Vector Machine</td>\n",
    "      <td id=\"T_f4a83_row5_col1\" class=\"data row5 col1\" >0.48</td>\n",
    "      <td id=\"T_f4a83_row5_col2\" class=\"data row5 col2\" >0.48</td>\n",
    "      <td id=\"T_f4a83_row5_col3\" class=\"data row5 col3\" >0.48</td>\n",
    "      <td id=\"T_f4a83_row5_col4\" class=\"data row5 col4\" >0.48</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_f4a83_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
    "      <td id=\"T_f4a83_row6_col0\" class=\"data row6 col0\" >Naive Bayes</td>\n",
    "      <td id=\"T_f4a83_row6_col1\" class=\"data row6 col1\" >0.45</td>\n",
    "      <td id=\"T_f4a83_row6_col2\" class=\"data row6 col2\" >0.47</td>\n",
    "      <td id=\"T_f4a83_row6_col3\" class=\"data row6 col3\" >0.45</td>\n",
    "      <td id=\"T_f4a83_row6_col4\" class=\"data row6 col4\" >0.45</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_f4a83_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
    "      <td id=\"T_f4a83_row7_col0\" class=\"data row7 col0\" >Stochastic Gradient Descent</td>\n",
    "      <td id=\"T_f4a83_row7_col1\" class=\"data row7 col1\" >0.46</td>\n",
    "      <td id=\"T_f4a83_row7_col2\" class=\"data row7 col2\" >0.46</td>\n",
    "      <td id=\"T_f4a83_row7_col3\" class=\"data row7 col3\" >0.46</td>\n",
    "      <td id=\"T_f4a83_row7_col4\" class=\"data row7 col4\" >0.45</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_f4a83_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
    "      <td id=\"T_f4a83_row8_col0\" class=\"data row8 col0\" >Gradient Boosting</td>\n",
    "      <td id=\"T_f4a83_row8_col1\" class=\"data row8 col1\" >0.44</td>\n",
    "      <td id=\"T_f4a83_row8_col2\" class=\"data row8 col2\" >0.44</td>\n",
    "      <td id=\"T_f4a83_row8_col3\" class=\"data row8 col3\" >0.44</td>\n",
    "      <td id=\"T_f4a83_row8_col4\" class=\"data row8 col4\" >0.43</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_f4a83_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
    "      <td id=\"T_f4a83_row9_col0\" class=\"data row9 col0\" >AdaBoost</td>\n",
    "      <td id=\"T_f4a83_row9_col1\" class=\"data row9 col1\" >0.35</td>\n",
    "      <td id=\"T_f4a83_row9_col2\" class=\"data row9 col2\" >0.36</td>\n",
    "      <td id=\"T_f4a83_row9_col3\" class=\"data row9 col3\" >0.35</td>\n",
    "      <td id=\"T_f4a83_row9_col4\" class=\"data row9 col4\" >0.35</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_f4a83_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
    "      <td id=\"T_f4a83_row10_col0\" class=\"data row10 col0\" >Quadratic Discriminant Analysis</td>\n",
    "      <td id=\"T_f4a83_row10_col1\" class=\"data row10 col1\" >0.17</td>\n",
    "      <td id=\"T_f4a83_row10_col2\" class=\"data row10 col2\" >0.2</td>\n",
    "      <td id=\"T_f4a83_row10_col3\" class=\"data row10 col3\" >0.17</td>\n",
    "      <td id=\"T_f4a83_row10_col4\" class=\"data row10 col4\" >0.15</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
